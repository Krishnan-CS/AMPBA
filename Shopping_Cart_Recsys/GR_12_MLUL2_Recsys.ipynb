{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvRglOkonVIu"
   },
   "source": [
    "# MLUL2 Group Assignment — “Did You Forget?” Recommendation System\n",
    "   \n",
    "**Team Members**:  \n",
    "- Akanksha Parab (12420067)  \n",
    "- Deepankar Garg (12420012)\n",
    "- Krishnan Chathadi S (12420068)\n",
    "- Shama Shilpi (12420033)\n",
    "\n",
    "**Objective**:  \n",
    "Design a recommendation system to predict items, which the customers may have forgotten to order, using past purchase behavior and partial last-order information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "07uJ3awunVIv"
   },
   "source": [
    "# Libraries and functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwr6ppHC1avX"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qRgSga-mH-K"
   },
   "source": [
    "If there are any library related issues when running this notebook in Colab, follow these steps:\n",
    "* Uncomment and run #!pip install \"numpy<2.0\"\n",
    "* Uncomment and run #!pip install scikit-surprise --no-binary :all:\n",
    "* Comment them out and restart the session.\n",
    "* Run the rest of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "id": "p-JkXBumjoxw",
    "outputId": "7ed67b91-947f-4907-ed65-ec5f9c28221b"
   },
   "outputs": [],
   "source": [
    "#!pip install \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivM5tJJvf382",
    "outputId": "7abb245f-0cae-4d0a-87cc-00ee3f6572e8"
   },
   "outputs": [],
   "source": [
    "#!pip install scikit-surprise --no-binary :all:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "id": "V5Kfa5oxnVIw",
    "outputId": "36f4e8d8-f8b8-4fd8-bb78-3ed2ac4417ff"
   },
   "outputs": [],
   "source": [
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Libraries for plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Libraries for handling pairs of data, dictionaries, datatime\n",
    "import itertools\n",
    "from itertools import combinations\n",
    "from collections import defaultdict, Counter\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler  # Scaling the data\n",
    "from scipy.stats import norm # Normal distribution\n",
    "\n",
    "# Matrix factorization\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYbpiXyj1h6q"
   },
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9ehsWkVu7J2G"
   },
   "outputs": [],
   "source": [
    "def assign_member_cluster(row):\n",
    "    \"\"\"Assigns a member cluster based on diversity score.\"\"\"\n",
    "\n",
    "    if row['Diversity'] <= 0.5:\n",
    "        return 'Repetitive'\n",
    "    elif row['Diversity'] >= 0.75 and row['Num_orders'] >= 3:\n",
    "        return 'Explorer'\n",
    "    elif row['Num_orders'] <= 2:\n",
    "        return 'NewUser'\n",
    "    elif row['Num_orders'] >= 6:\n",
    "        return 'FrequentBuyer'\n",
    "    else:\n",
    "        return 'Normal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TxS9WjBDbF92"
   },
   "outputs": [],
   "source": [
    "def assign_season(month):\n",
    "    \"\"\"Assigns a season based on the month.\"\"\"\n",
    "    if month in [3, 4, 5]:\n",
    "        return \"Summer\"\n",
    "    elif month in [6, 7, 8, 9, 10]:\n",
    "        return \"Monsoon\"\n",
    "    else:\n",
    "        return \"Winter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "n4rMdXhauSiE"
   },
   "outputs": [],
   "source": [
    "# Function to split SKUs for a single order\n",
    "def split_order_skus(sku_list, frac=0.3):\n",
    "    \"\"\"Splits a list of SKUs into observed and withheld sets.\"\"\"\n",
    "\n",
    "    skus = list(sku_list)  # Ensure it's a list\n",
    "    np.random.shuffle(skus)  # Shuffle the SKUs\n",
    "    split_point = max(1, int(len(skus) * frac))  # Ensure at least 1 SKU in observed\n",
    "\n",
    "    observed = skus[:split_point]\n",
    "    withheld = skus[split_point:]\n",
    "    if len(withheld) > 5:\n",
    "        withheld = withheld[:5]\n",
    "    return observed, withheld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QHRLm-h-DEok"
   },
   "outputs": [],
   "source": [
    "def compute_recall_at_5(df_predictions: pd.DataFrame, df_withheld: pd.DataFrame) -> float:\n",
    "    \"\"\"Computes Recall@5 for predictions against withheld items.\"\"\"\n",
    "\n",
    "    # Convert to sets grouped by Order\n",
    "    pred_dict = df_predictions.groupby(\"Order\")[\"SKU\"].apply(set).to_dict()\n",
    "    true_dict = df_withheld.groupby(\"Order\")[\"SKU\"].apply(set).to_dict()\n",
    "\n",
    "    recall_scores = []\n",
    "\n",
    "    # Compute the recall values\n",
    "    for order_id, true_skus in true_dict.items():\n",
    "        pred_skus = pred_dict.get(order_id, set())\n",
    "        if not true_skus:\n",
    "            continue  # Skip if no ground truth\n",
    "        recall = len(true_skus & pred_skus) / len(true_skus)\n",
    "        recall_scores.append(recall)\n",
    "\n",
    "    # Average across all orders\n",
    "    return sum(recall_scores) / len(recall_scores) if recall_scores else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_recall_evaluation(combined_preds, df_withheld, strategy_mode=False):\n",
    "    results = []\n",
    "\n",
    "    # Cluster-Level Evaluation\n",
    "    for cluster_name, df_pred in combined_preds.items():\n",
    "        df_truth = df_withheld[df_withheld['Member'].isin(df_pred['Member'].unique())]\n",
    "        recall = compute_recall_at_5(df_pred, df_truth)\n",
    "\n",
    "        results.append({\n",
    "            'Level': 'Cluster',\n",
    "            'Segment': cluster_name,\n",
    "            'Recall@5': round(recall, 4)\n",
    "        })\n",
    "\n",
    "    # Overall Evaluation\n",
    "    full_df = pd.concat(combined_preds.values(), ignore_index=True)\n",
    "    full_truth = df_withheld[df_withheld['Member'].isin(full_df['Member'].unique())]\n",
    "\n",
    "    overall_recall = compute_recall_at_5(full_df, full_truth)\n",
    "    results.append({\n",
    "        'Level': 'Overall',\n",
    "        'Segment': 'All',\n",
    "        'Recall@5': round(overall_recall, 4)\n",
    "    })\n",
    "\n",
    "    # Strategy-Level Evaluation\n",
    "    if strategy_mode:\n",
    "        for label, df_pred in zip(pred_labels, df_preds):  # assuming these exist\n",
    "            cluster = label.split(\"_\")[0]\n",
    "            df_truth = df_withheld[df_withheld['Member'].isin(df_pred['Member'].unique())]\n",
    "            recall = compute_recall_at_5(df_pred, df_truth)\n",
    "\n",
    "            results.append({\n",
    "                'Level': 'Strategy',\n",
    "                'Segment': cluster,\n",
    "                'Strategy': label.split(\"_\")[1],\n",
    "                'Recall@5': round(recall, 4)\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_of_truth(df_pred, df_truth):\n",
    "    # Merge predicted and true SKUs per Member & Order\n",
    "    merged = df_pred.copy()\n",
    "    merged['Predicted_Rank'] = (\n",
    "        merged\n",
    "        .sort_values(by=['Member', 'Combined_Score'], ascending=[True, False])\n",
    "        .groupby(['Member', 'Order'])\n",
    "        .cumcount() + 1  # 1-based indexing\n",
    "    )\n",
    "\n",
    "    # Label if SKU is a forgotten item (i.e., in withheld truth)\n",
    "    merged['Is_Hit'] = merged.set_index(['Member', 'Order', 'SKU']).index.isin(\n",
    "        df_truth.set_index(['Member', 'Order', 'SKU']).index\n",
    "    )\n",
    "\n",
    "    # Filter only hits and capture their rank\n",
    "    hits_df = merged[merged['Is_Hit']]\n",
    "    return hits_df[['Member', 'Order', 'SKU', 'Predicted_Rank']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "B3_w3M2YLMiQ"
   },
   "outputs": [],
   "source": [
    "def build_jaccard_matrix(orders_df):\n",
    "    \"\"\"\n",
    "    Takes a DataFrame with columns ['Order', 'SKU'] and returns a nested dict:\n",
    "    jaccard_matrix[sku1][sku2] = Jaccard similarity between sku1 and sku2\n",
    "    \"\"\"\n",
    "    # Step 1: Build item → set of orders\n",
    "    sku_to_orders = defaultdict(set)\n",
    "\n",
    "    for row in orders_df.itertuples(index=False):\n",
    "        sku_to_orders[row.SKU].add(row.Order)\n",
    "\n",
    "    all_skus = list(sku_to_orders.keys())\n",
    "    jaccard_matrix = defaultdict(dict)\n",
    "\n",
    "    # Step 2: Loop through combinations and compute Jaccard\n",
    "    for sku1, sku2 in itertools.combinations(all_skus, 2):\n",
    "        orders1 = sku_to_orders[sku1]\n",
    "        orders2 = sku_to_orders[sku2]\n",
    "        intersection = len(orders1 & orders2)\n",
    "        union = len(orders1 | orders2)\n",
    "\n",
    "        if union > 0:\n",
    "            score = intersection / union\n",
    "            if score > 0:\n",
    "                jaccard_matrix[sku1][sku2] = score\n",
    "                jaccard_matrix[sku2][sku1] = score  # symmetry\n",
    "\n",
    "    return jaccard_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDfSujTwnVIx"
   },
   "source": [
    "# 1. Data loading and exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRage5MPnVIx"
   },
   "source": [
    "## Load the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "F_xUHca7nVIx"
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "all_except_last_orders_df = pd.read_csv('all_except_last_orders.csv')\n",
    "last_orders_subset_df = pd.read_csv('last_orders_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3LdA0ZMGnVIx"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order</th>\n",
       "      <th>SKU</th>\n",
       "      <th>Member</th>\n",
       "      <th>Delivery Date</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8358896</td>\n",
       "      <td>15668375</td>\n",
       "      <td>SSCEHNS</td>\n",
       "      <td>02/11/13</td>\n",
       "      <td>Root Vegetables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8358896</td>\n",
       "      <td>15668467</td>\n",
       "      <td>SSCEHNS</td>\n",
       "      <td>02/11/13</td>\n",
       "      <td>Beans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8358896</td>\n",
       "      <td>15669863</td>\n",
       "      <td>SSCEHNS</td>\n",
       "      <td>02/11/13</td>\n",
       "      <td>Moong Dal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8358896</td>\n",
       "      <td>15669778</td>\n",
       "      <td>SSCEHNS</td>\n",
       "      <td>02/11/13</td>\n",
       "      <td>Other Dals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8358896</td>\n",
       "      <td>15669767</td>\n",
       "      <td>SSCEHNS</td>\n",
       "      <td>02/11/13</td>\n",
       "      <td>Urad Dal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28979</th>\n",
       "      <td>7466404</td>\n",
       "      <td>15669886</td>\n",
       "      <td>SWRNHCS</td>\n",
       "      <td>01/09/13</td>\n",
       "      <td>Sooji &amp; Rava</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28980</th>\n",
       "      <td>7466404</td>\n",
       "      <td>15669874</td>\n",
       "      <td>SWRNHCS</td>\n",
       "      <td>01/09/13</td>\n",
       "      <td>Avalakki / Poha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28981</th>\n",
       "      <td>7466404</td>\n",
       "      <td>15670260</td>\n",
       "      <td>SWRNHCS</td>\n",
       "      <td>01/09/13</td>\n",
       "      <td>Organic F&amp;V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28982</th>\n",
       "      <td>7466404</td>\n",
       "      <td>15670196</td>\n",
       "      <td>SWRNHCS</td>\n",
       "      <td>01/09/13</td>\n",
       "      <td>Organic F&amp;V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28983</th>\n",
       "      <td>7466404</td>\n",
       "      <td>34986129</td>\n",
       "      <td>SWRNHCS</td>\n",
       "      <td>01/09/13</td>\n",
       "      <td>Whole Spices</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28984 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Order       SKU   Member Delivery Date             Name\n",
       "0      8358896  15668375  SSCEHNS      02/11/13  Root Vegetables\n",
       "1      8358896  15668467  SSCEHNS      02/11/13            Beans\n",
       "2      8358896  15669863  SSCEHNS      02/11/13        Moong Dal\n",
       "3      8358896  15669778  SSCEHNS      02/11/13       Other Dals\n",
       "4      8358896  15669767  SSCEHNS      02/11/13         Urad Dal\n",
       "...        ...       ...      ...           ...              ...\n",
       "28979  7466404  15669886  SWRNHCS      01/09/13     Sooji & Rava\n",
       "28980  7466404  15669874  SWRNHCS      01/09/13  Avalakki / Poha\n",
       "28981  7466404  15670260  SWRNHCS      01/09/13      Organic F&V\n",
       "28982  7466404  15670196  SWRNHCS      01/09/13      Organic F&V\n",
       "28983  7466404  34986129  SWRNHCS      01/09/13     Whole Spices\n",
       "\n",
       "[28984 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the data\n",
    "all_except_last_orders_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "H1JlpVzunVIx"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order</th>\n",
       "      <th>SKU</th>\n",
       "      <th>Member</th>\n",
       "      <th>Delivery Date</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7409204</td>\n",
       "      <td>15669778</td>\n",
       "      <td>SWLCNOE</td>\n",
       "      <td>05/09/13</td>\n",
       "      <td>Other Dals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8076206</td>\n",
       "      <td>15669977</td>\n",
       "      <td>SWOEZES</td>\n",
       "      <td>01/04/14</td>\n",
       "      <td>Almonds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7560723</td>\n",
       "      <td>7593949</td>\n",
       "      <td>SSWWRHW</td>\n",
       "      <td>30/06/13</td>\n",
       "      <td>Cream Biscuits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8362837</td>\n",
       "      <td>15669764</td>\n",
       "      <td>SWLSCOZ</td>\n",
       "      <td>06/11/13</td>\n",
       "      <td>Besan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8202458</td>\n",
       "      <td>15670196</td>\n",
       "      <td>SSRCRSO</td>\n",
       "      <td>03/02/14</td>\n",
       "      <td>Organic F&amp;V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5482</th>\n",
       "      <td>8269882</td>\n",
       "      <td>15668469</td>\n",
       "      <td>SWNHZNW</td>\n",
       "      <td>05/01/14</td>\n",
       "      <td>Beans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5483</th>\n",
       "      <td>8384422</td>\n",
       "      <td>15669875</td>\n",
       "      <td>SSWNRHC</td>\n",
       "      <td>18/11/13</td>\n",
       "      <td>Toor Dal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5484</th>\n",
       "      <td>7493590</td>\n",
       "      <td>15668465</td>\n",
       "      <td>SWRELHW</td>\n",
       "      <td>07/08/13</td>\n",
       "      <td>Root Vegetables</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5485</th>\n",
       "      <td>8080319</td>\n",
       "      <td>15670267</td>\n",
       "      <td>SSNSECH</td>\n",
       "      <td>03/04/14</td>\n",
       "      <td>Toor Dal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5486</th>\n",
       "      <td>8215172</td>\n",
       "      <td>15669954</td>\n",
       "      <td>SWNLHOO</td>\n",
       "      <td>08/02/14</td>\n",
       "      <td>Other Dry Fruits</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5487 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Order       SKU   Member Delivery Date              Name\n",
       "0     7409204  15669778  SWLCNOE      05/09/13        Other Dals\n",
       "1     8076206  15669977  SWOEZES      01/04/14           Almonds\n",
       "2     7560723   7593949  SSWWRHW      30/06/13    Cream Biscuits\n",
       "3     8362837  15669764  SWLSCOZ      06/11/13             Besan\n",
       "4     8202458  15670196  SSRCRSO      03/02/14       Organic F&V\n",
       "...       ...       ...      ...           ...               ...\n",
       "5482  8269882  15668469  SWNHZNW      05/01/14             Beans\n",
       "5483  8384422  15669875  SSWNRHC      18/11/13          Toor Dal\n",
       "5484  7493590  15668465  SWRELHW      07/08/13   Root Vegetables\n",
       "5485  8080319  15670267  SSNSECH      03/04/14          Toor Dal\n",
       "5486  8215172  15669954  SWNLHOO      08/02/14  Other Dry Fruits\n",
       "\n",
       "[5487 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Check the data\n",
    "last_orders_subset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xy196YounVIy"
   },
   "source": [
    "## Check info, nulls and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "RqkOtp8dnVIy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28984 entries, 0 to 28983\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Order          28984 non-null  int64 \n",
      " 1   SKU            28984 non-null  int64 \n",
      " 2   Member         28984 non-null  object\n",
      " 3   Delivery Date  28984 non-null  object\n",
      " 4   Name           28984 non-null  object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "all_except_last_orders_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "WPD1M-AUnVIy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_except_last_orders_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "LCIHYQwXnVIy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5487 entries, 0 to 5486\n",
      "Data columns (total 5 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Order          5487 non-null   int64 \n",
      " 1   SKU            5487 non-null   int64 \n",
      " 2   Member         5487 non-null   object\n",
      " 3   Delivery Date  5487 non-null   object\n",
      " 4   Name           5487 non-null   object\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 214.5+ KB\n"
     ]
    }
   ],
   "source": [
    "last_orders_subset_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "BuYynXRrnVIy"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_orders_subset_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dFhwHksBnVIz"
   },
   "source": [
    "### Summary\n",
    "- Rows and columns in each file:\n",
    "- **all_except_last_orders**: 28984 rows, 5 columns, no null or duplicates.\n",
    "- **last_orders_subset**: 5487 rows, 5 columns, no null or duplicates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruB_4AqBnVIy"
   },
   "source": [
    "## Get counts of unique members, SKUs and orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Y5OOFGZ4nVIy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In all_except_last_orders: \n",
      "\n",
      "Number of unique members: 638\n",
      "Number of unique SKUs: 632\n",
      "Number of unique orders: 2595\n"
     ]
    }
   ],
   "source": [
    "print('In all_except_last_orders: \\n')\n",
    "print('Number of unique members:', len(all_except_last_orders_df['Member'].unique()))\n",
    "print('Number of unique SKUs:', len(all_except_last_orders_df['SKU'].unique()))\n",
    "print('Number of unique orders:', len(all_except_last_orders_df['Order'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yLK4DMYdveZ6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In last_orders_subset: \n",
      "\n",
      "Number of unique members: 638\n",
      "Number of unique SKUs: 565\n",
      "Number of unique orders: 638\n"
     ]
    }
   ],
   "source": [
    "print('In last_orders_subset: \\n')\n",
    "print('Number of unique members:', len(last_orders_subset_df['Member'].unique()))\n",
    "print('Number of unique SKUs:', len(last_orders_subset_df['SKU'].unique()))\n",
    "print('Number of unique orders:', len(last_orders_subset_df['Order'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPzhPwbBnVIz"
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "Questions to Explore:\n",
    "- How do the SKUs and names map to each other?\n",
    "- SKU popularity.\n",
    "- Which SKUs are most frequently bought together?\n",
    "- Customer behaviour insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DQlPvV9nVIz"
   },
   "source": [
    "### How do the SKUs and names map to each other?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "V4mOt4iunVIz"
   },
   "outputs": [],
   "source": [
    "# Check the mapping between SKU and name to get an idea on the product\n",
    "sku_name_map = all_except_last_orders_df.set_index(\"SKU\")[\"Name\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4iTyzPQynVIz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are there SKUs with more than one name?\n",
    "sku_name_counts = all_except_last_orders_df.groupby('SKU')['Name'].nunique()\n",
    "multi_name_skus = sku_name_counts[sku_name_counts > 1]\n",
    "len(multi_name_skus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "jJCLOb0TnVIz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Are there names which map to multiple SKUs?\n",
    "name_sku_counts = all_except_last_orders_df.groupby('Name')['SKU'].nunique()\n",
    "ambiguous_names = name_sku_counts[name_sku_counts > 1]\n",
    "len(ambiguous_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHsTpXCHnVIz"
   },
   "source": [
    "#### SKU-Name mapping summary\n",
    "- A particular product name can have multiple SKU. This situation arises because different sizes, flavours of the same product are tagged as different SKUs.\n",
    "- None of the SKUs map to multiple products.\n",
    "- The recommendations would be based on SKUs.\n",
    "- The name column would only be useful for labelling and plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-OWINC8ky1k5"
   },
   "source": [
    "### SKU Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ifoXNVsWyzbY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SKU\n",
       "15668381    530\n",
       "15668688    482\n",
       "15668460    479\n",
       "15668379    479\n",
       "15669878    445\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SKU frequency - to understand SKU popularity\n",
    "sku_counts = all_except_last_orders_df['SKU'].value_counts()\n",
    "sku_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3FnXDuKKzBR"
   },
   "source": [
    "### Which SKUs are frequently bought together?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "H3svLAIyK3M0"
   },
   "outputs": [],
   "source": [
    "# Initialize the global jaccard matrix. Not printing the output, since it could crash the runtime.\n",
    "global_jaccard_matrix = build_jaccard_matrix(all_except_last_orders_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "THjL_iginVI0"
   },
   "source": [
    "### Customer behaviour insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "wuh2J5DDnVI0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    638.000000\n",
       "mean       4.067398\n",
       "std        2.958133\n",
       "min        1.000000\n",
       "25%        2.000000\n",
       "50%        3.000000\n",
       "75%        6.000000\n",
       "max       16.000000\n",
       "Name: Order, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution of number of orders per member: Useful to filter active users\n",
    "all_except_last_orders_df.groupby('Member')['Order'].nunique().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "tSrsWtdHnVI0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2595.000000\n",
       "mean       11.169171\n",
       "std         3.375311\n",
       "min         8.000000\n",
       "25%         9.000000\n",
       "50%        10.000000\n",
       "75%        13.000000\n",
       "max        31.000000\n",
       "Name: SKU, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cart size (SKUs per order)\n",
    "all_except_last_orders_df.groupby('Order')['SKU'].count().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdRt_wH_2DDp"
   },
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "fopFaKsd2Gvg"
   },
   "outputs": [],
   "source": [
    "# Create copies of dataset before modifications\n",
    "df_all = all_except_last_orders_df.copy()\n",
    "df_last = last_orders_subset_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "EdXvMMdZdrAr"
   },
   "outputs": [],
   "source": [
    "# Convert delivery date to date time format\n",
    "df_all['Delivery Date'] = pd.to_datetime(df_all['Delivery Date'], format='%d/%m/%y')\n",
    "df_last['Delivery Date'] = pd.to_datetime(df_last['Delivery Date'], format='%d/%m/%y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ogTg_GZ0ecvt"
   },
   "outputs": [],
   "source": [
    "df_all[\"Month\"] = df_all[\"Delivery Date\"].dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ogwSwBnDQjk1"
   },
   "source": [
    "### Create Member Stats Dataframe to store key data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "dSaJJ17MQo0R"
   },
   "outputs": [],
   "source": [
    "# Create Member_Stats with number of unique SKUs and number of unique orders from df_all\n",
    "member_stats_df = df_all.groupby('Member').agg(\n",
    "    Unique_SKUs=('SKU', 'nunique'),\n",
    "    Total_SKUs=('SKU', 'count'),\n",
    "    Num_orders=('Order', 'nunique')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "QHOQRVW4Qqx4"
   },
   "outputs": [],
   "source": [
    "member_stats_df['Diversity'] = member_stats_df['Unique_SKUs'] / member_stats_df['Total_SKUs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "s7_pU2hfZVCT"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unique_SKUs</th>\n",
       "      <th>Total_SKUs</th>\n",
       "      <th>Num_orders</th>\n",
       "      <th>Diversity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>638.000000</td>\n",
       "      <td>638.000000</td>\n",
       "      <td>638.000000</td>\n",
       "      <td>638.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>28.827586</td>\n",
       "      <td>45.429467</td>\n",
       "      <td>4.067398</td>\n",
       "      <td>0.772037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17.041230</td>\n",
       "      <td>39.761462</td>\n",
       "      <td>2.958133</td>\n",
       "      <td>0.192050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.636693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>25.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.802174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>110.000000</td>\n",
       "      <td>347.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unique_SKUs  Total_SKUs  Num_orders   Diversity\n",
       "count   638.000000  638.000000  638.000000  638.000000\n",
       "mean     28.827586   45.429467    4.067398    0.772037\n",
       "std      17.041230   39.761462    2.958133    0.192050\n",
       "min       8.000000    8.000000    1.000000    0.230548\n",
       "25%      16.000000   18.000000    2.000000    0.636693\n",
       "50%      25.500000   32.000000    3.000000    0.802174\n",
       "75%      38.000000   62.000000    6.000000    0.944444\n",
       "max     110.000000  347.000000   16.000000    1.000000"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "member_stats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "_X_8jttirHqv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Member_Cluster\n",
       "Explorer         134\n",
       "FrequentBuyer     97\n",
       "NewUser          249\n",
       "Normal            91\n",
       "Repetitive        67\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "member_stats_df['Member_Cluster'] = member_stats_df.apply(assign_member_cluster, axis=1)\n",
    "member_stats_df.groupby('Member_Cluster').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bU4pHq1Zhnr"
   },
   "source": [
    "### Create Member-Order dataframe to analyze SKU frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "eL3acSa4lT0d"
   },
   "outputs": [],
   "source": [
    "# Group by member and order, aggregate SKUs into lists\n",
    "member_order_df = (\n",
    "    df_all\n",
    "    .groupby(['Member', 'Order', 'Delivery Date'])['SKU']\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    "    .sort_values(by=['Member', 'Delivery Date'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "CECNHj-wlqTx"
   },
   "outputs": [],
   "source": [
    "# Explode SKU list to get one SKU per row\n",
    "sku_order_df = member_order_df.explode('SKU')\n",
    "\n",
    "# Sort for time-differencing\n",
    "sku_order_df['Date'] = pd.to_datetime(sku_order_df['Delivery Date'], format='%d/%m/%y')\n",
    "sku_order_df = sku_order_df.sort_values(by=['Member', 'SKU', 'Date'])\n",
    "\n",
    "# Compute days between repeated purchases of same SKU by the same member\n",
    "sku_order_df['Days_Since_Last_Purchase'] = (\n",
    "    sku_order_df.groupby(['Member', 'SKU'])['Date']\n",
    "    .diff().dt.days\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "aOUMNtJanLX1"
   },
   "outputs": [],
   "source": [
    "# Create a table for SKU periodicity\n",
    "sku_periodicity = (\n",
    "    sku_order_df[sku_order_df['Days_Since_Last_Purchase'].notnull()]\n",
    "    .groupby(['Member', 'SKU'])['Days_Since_Last_Purchase']\n",
    "    .agg(['mean', 'std'])\n",
    "    .reset_index()\n",
    "    .rename(columns={'mean': 'Typical_Gap', 'std': 'Gap_STD'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SKU</th>\n",
       "      <th>Typical_Gap</th>\n",
       "      <th>Gap_STD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.468000e+03</td>\n",
       "      <td>5468.000000</td>\n",
       "      <td>2410.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.623441e+07</td>\n",
       "      <td>81.130739</td>\n",
       "      <td>44.372903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.034437e+07</td>\n",
       "      <td>79.383242</td>\n",
       "      <td>50.420225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.541573e+06</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.566846e+07</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>13.435029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.566977e+07</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>27.577164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.566986e+07</td>\n",
       "      <td>99.541667</td>\n",
       "      <td>55.921586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.328949e+07</td>\n",
       "      <td>692.000000</td>\n",
       "      <td>396.686904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                SKU  Typical_Gap      Gap_STD\n",
       "count  5.468000e+03  5468.000000  2410.000000\n",
       "mean   1.623441e+07    81.130739    44.372903\n",
       "std    1.034437e+07    79.383242    50.420225\n",
       "min    7.541573e+06     3.000000     0.000000\n",
       "25%    1.566846e+07    32.000000    13.435029\n",
       "50%    1.566977e+07    55.000000    27.577164\n",
       "75%    1.566986e+07    99.541667    55.921586\n",
       "max    9.328949e+07   692.000000   396.686904"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sku_periodicity.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "dGLOhaJ43Mb4"
   },
   "outputs": [],
   "source": [
    "# Apply a Z-score based scoring\n",
    "sku_scored_df = sku_order_df.merge(sku_periodicity, on=['Member', 'SKU'], how='left')\n",
    "\n",
    "sku_scored_df['Overdue_Z'] = (\n",
    "    (sku_scored_df['Days_Since_Last_Purchase'] - sku_scored_df['Typical_Gap']) /\n",
    "    sku_scored_df['Gap_STD']\n",
    ") # Leave NaNs as-is — avoid fillna(0) to prevent false positives\n",
    "\n",
    "# Multiple cdf by 2, so that the value likes between 0 and 1\n",
    "sku_scored_df['Periodicity_Score'] = 2 * (1 - norm.cdf(np.abs(sku_scored_df['Overdue_Z'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YucQz3rOdFlp"
   },
   "source": [
    "### Seasonal Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "KrUrl6rhp0hl"
   },
   "outputs": [],
   "source": [
    "df_all[\"Season_Group\"] = df_all[\"Delivery Date\"].dt.month.map(assign_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "2UqCLUyfdEjJ"
   },
   "outputs": [],
   "source": [
    "# Quick test season_group instead of month\n",
    "sku_month_counts = (\n",
    "    df_all\n",
    "    .groupby([\"SKU\", \"Season_Group\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"Count\")\n",
    ")\n",
    "\n",
    "sku_month_dist = sku_month_counts.pivot_table(\n",
    "    index=\"SKU\",\n",
    "    columns=\"Season_Group\",\n",
    "    values=\"Count\",\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Normalize row-wise to get distribution\n",
    "sku_month_dist = sku_month_dist.div(sku_month_dist.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlsd3M3Jqzai"
   },
   "source": [
    "### Create dictionaries for Jaccard scores in member carts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "OGcRZ54p3h9O"
   },
   "outputs": [],
   "source": [
    "# Co-occurrence counts and marginal SKU counts\n",
    "member_co_counts = defaultdict(lambda: defaultdict(int))\n",
    "member_sku_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Loop over each order\n",
    "for _, row in member_order_df.iterrows():\n",
    "    member = row['Member']\n",
    "    sku_list = set(row['SKU'])  # remove duplicates\n",
    "\n",
    "    # Update marginal counts\n",
    "    for sku in sku_list:\n",
    "        member_sku_counts[member][sku] += 1\n",
    "\n",
    "    # Update co-occurrence counts\n",
    "    for sku_i, sku_j in combinations(sku_list, 2):\n",
    "        member_co_counts[member][(sku_i, sku_j)] += 1\n",
    "        member_co_counts[member][(sku_j, sku_i)] += 1  # symmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "rpiCG7L3rTGi"
   },
   "outputs": [],
   "source": [
    "# Create the dictionary of SKU co-occurrence for different members\n",
    "member_jaccard = defaultdict(dict)\n",
    "\n",
    "# Store member jaccard scores\n",
    "for member in member_co_counts:\n",
    "    for (sku_i, sku_j), co_count in member_co_counts[member].items():\n",
    "        count_i = member_sku_counts[member][sku_i]\n",
    "        count_j = member_sku_counts[member][sku_j]\n",
    "        union = count_i + count_j - co_count\n",
    "\n",
    "        if union > 0:\n",
    "            jaccard_score = co_count / union\n",
    "            member_jaccard[member][(sku_i, sku_j)] = jaccard_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKI2RRYmnVI4"
   },
   "source": [
    "# 3. Recommendation Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EksxcXATnVI4"
   },
   "source": [
    "### Approach Considered\n",
    "- Candidate Generation: Based on historical SKUs\n",
    "  * The goal is to identify items a user has purchased in the past but did not include in their latest order.\n",
    "  * The list of items considered for 'Did you forget?' would be: (All past orders by member - Orders in the cart for that member)\n",
    "  * Create a fallback pool, if there are less than 5 candidates. This could be a list of most popular SKUs (overall or recently popular)\n",
    "- Ranking the candidates to generate top 5.\n",
    "  * Start with simple frequency based ranking.\n",
    "  * Consider other factors like co-occurance, periodicity, seasonality etc.\n",
    "  * Create a weighted ranking score based on all the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wRi4Eaz5TW79"
   },
   "source": [
    "### Candidate generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "l5OsG2UcTbtD"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary with a list of SKUs for each member, sorted by frequency and made unique\n",
    "def get_member_sku_dict(df):\n",
    "    \"\"\"Returns a dictionary of member to a list of unique SKUs sorted by frequency.\"\"\"\n",
    "\n",
    "    member_sku_dict = {}\n",
    "    grouped = df.groupby('Member')['SKU'].apply(list)\n",
    "\n",
    "    for member, skus in grouped.items():\n",
    "        # Count SKU frequencies for the current member\n",
    "        sku_counts = Counter(skus)\n",
    "\n",
    "        # Sort SKUs by frequency in descending order, then get unique SKUs in that order\n",
    "        sorted_unique_skus = []\n",
    "        for sku in sorted(skus, key=lambda sku: sku_counts[sku], reverse=True):\n",
    "            if sku not in sorted_unique_skus:\n",
    "                sorted_unique_skus.append(sku)\n",
    "        member_sku_dict[member] = sorted_unique_skus\n",
    "    return member_sku_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "703rrz8iRiqC"
   },
   "source": [
    "### Fallback pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "vS0zp_vMQyzV"
   },
   "outputs": [],
   "source": [
    "# For fallback pool - If there are less than 5 recommendations for a order, include additional SKUs from these.\n",
    "def get_top_skus(df, top_n=5):\n",
    "    \"\"\"Returns the top N most frequent SKUs in the DataFrame.\"\"\"\n",
    "\n",
    "    top_skus = df_all['SKU'].value_counts().head(5).index.tolist()\n",
    "    return top_skus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILYmaPX9jyez"
   },
   "source": [
    "### Member Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "qrr3g3YId3--"
   },
   "outputs": [],
   "source": [
    "def jaccard_cart_reranker(last_orders_df, test_mode=False, k=12):\n",
    "    \"\"\"\n",
    "    Reranks recommendations based on member-specific Jaccard similarity.\n",
    "    Input: Last orders in Dataframe format.\n",
    "    Output: Recommendations in Dataframe format.\n",
    "    \"\"\"\n",
    "\n",
    "    recommendations = []\n",
    "    df = df_input if test_mode else df_all\n",
    "    mem_sku_all = get_member_sku_dict(df)\n",
    "\n",
    "    # Group input DataFrame by Order and Member\n",
    "    grouped_orders = last_orders_df.groupby(['Order', 'Member'])['SKU'].apply(set).to_dict()\n",
    "\n",
    "    for (order_id, member_id), current_skus_set in grouped_orders.items():\n",
    "        score_dict = defaultdict(float)\n",
    "\n",
    "        for (sku_a, sku_b), jscore in member_jaccard.get(member_id, {}).items():\n",
    "            # If sku_a is in current cart and sku_b is not, we consider sku_b as a candidate\n",
    "            if sku_a in current_skus_set and sku_b not in current_skus_set:\n",
    "                if sku_b in mem_sku_all.get(member_id, []):\n",
    "                    score_dict[sku_b] += jscore\n",
    "\n",
    "        sorted_candidates = sorted(score_dict.items(), key=lambda x: -x[1])[:k]\n",
    "        for sku, score in sorted_candidates:\n",
    "            recommendations.append({'Order': order_id, 'Member': member_id, 'SKU': sku, 'Member_Jaccard_Score': score})\n",
    "\n",
    "    return pd.DataFrame(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue-jiSa9j2BA"
   },
   "source": [
    "### Global Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "uHec4RSZj73p"
   },
   "outputs": [],
   "source": [
    "def jaccard_recommendations(last_orders_df, jaccard_matrix, test_mode=False, k=12):\n",
    "    \"\"\"\n",
    "    Generates recommendations based on global Jaccard similarity.\n",
    "    Input: Last orders in Dataframe format, Jaccard matrix.\n",
    "    Output: Recommendations in Dataframe format.\n",
    "    \"\"\"\n",
    "    all_recs = []\n",
    "\n",
    "    # Ensure 'Cart_SKUs' is available. Create it if necessary\n",
    "    if 'Cart_SKUs' not in last_orders_df.columns:\n",
    "        last_orders_df = last_orders_df.groupby(['Order', 'Member'])['SKU'].apply(set).reset_index(name='Cart_SKUs')\n",
    "\n",
    "    for _, row in last_orders_df.iterrows():\n",
    "        member_id = row['Member']\n",
    "        order_id = row['Order']\n",
    "        cart = row['Cart_SKUs']\n",
    "\n",
    "        candidate_scores = defaultdict(float)\n",
    "        for sku in cart:\n",
    "            neighbors = jaccard_matrix.get(sku, {})\n",
    "            for neighbor, score in neighbors.items():\n",
    "                if neighbor not in cart:\n",
    "                    candidate_scores[neighbor] += score\n",
    "\n",
    "        sorted_candidates = sorted(candidate_scores.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "        for sku, score in sorted_candidates:\n",
    "            all_recs.append({'Member': member_id, 'Order': order_id, 'SKU': sku, 'Global_Jaccard_Score': score})\n",
    "\n",
    "    return pd.DataFrame(all_recs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeI4FD5D544R"
   },
   "source": [
    "### Periodicity ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "ZohM9nHO53ul"
   },
   "outputs": [],
   "source": [
    "def periodicity_reranker(last_orders_df, test_mode=False, k=8):\n",
    "    \"\"\"\n",
    "    Reranks recommendations based on SKU purchase periodicity.\n",
    "    Input: Last orders in Dataframe format.\n",
    "    Output: Recommendations in Dataframe format.\n",
    "    \"\"\"\n",
    "\n",
    "    recommendations = []\n",
    "    df = df_input if test_mode else df_all\n",
    "    mem_sku_all = get_member_sku_dict(df)\n",
    "\n",
    "    # Group input DataFrame by Order and Member\n",
    "    grouped_orders = last_orders_df.groupby(['Order', 'Member'])['SKU'].apply(set).to_dict()\n",
    "\n",
    "    for (order_id, member_id), current_skus_set in grouped_orders.items():\n",
    "        past_skus = mem_sku_all[member_id]\n",
    "        candidate_skus = [sku for sku in past_skus if sku not in current_skus_set]\n",
    "\n",
    "        # Get latest Overdue_Z scores for candidate SKUs\n",
    "        candidate_scores = sku_scored_df[\n",
    "            (sku_scored_df['Member'] == member_id) &\n",
    "            (sku_scored_df['SKU'].isin(candidate_skus)) &\n",
    "            (sku_scored_df['Periodicity_Score'].notnull())\n",
    "        ].sort_values(by='Periodicity_Score', ascending=False).head(k)\n",
    "\n",
    "        # Append all candidates with their scores\n",
    "        if (len(candidate_scores) == 0):\n",
    "            recommendations.append({'Order': order_id, 'Member': member_id, 'SKU': past_skus[0], 'Periodicity_Score': 0.0})\n",
    "        else:\n",
    "            for index, row in candidate_scores.iterrows():\n",
    "                recommendations.append({'Order': order_id, 'Member': member_id, 'SKU': row['SKU'], 'Periodicity_Score': row['Periodicity_Score']})\n",
    "\n",
    "    return pd.DataFrame(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-36PcRBbRqVZ"
   },
   "source": [
    "### History based recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "id": "rPNllrAVRvGN"
   },
   "outputs": [],
   "source": [
    "def history_based_recommendations(last_orders_df, test_mode=False, k=15):\n",
    "    \"\"\"\n",
    "    Generates recommendations based on member's purchase history frequency.\n",
    "    Input: Last orders in Dataframe format.\n",
    "    Output: Recommendations in Dataframe format.\n",
    "    \"\"\"\n",
    "\n",
    "    recommendations = []\n",
    "    df = df_input if test_mode else df_all\n",
    "    mem_sku_all = get_member_sku_dict(df)\n",
    "\n",
    "    # Group input DataFrame by Order and Member\n",
    "    grouped_orders = last_orders_df.groupby(['Order', 'Member'])['SKU'].apply(set).to_dict()\n",
    "\n",
    "    for (order_id, member_id), current_skus_set in grouped_orders.items():\n",
    "        # Retrieve the member's full purchase history (SKUs ordered by frequency) from the dictionary\n",
    "        past_skus = mem_sku_all[member_id]\n",
    "\n",
    "        # Find forgotten items (in history but not in current order), maintaining frequency order\n",
    "        forgotten_skus = [sku for sku in past_skus if sku not in current_skus_set]\n",
    "\n",
    "        # Assign a frequency-based score (higher for more frequent)\n",
    "        sku_freq = Counter(df[df['Member'] == member_id]['SKU'])\n",
    "        scored_forgotten_skus = [(sku, sku_freq[sku]) for sku in forgotten_skus]\n",
    "\n",
    "        # Extract raw scores\n",
    "        #raw_scores = [score for _, score in scored_forgotten_skus]\n",
    "        #max_score = max(raw_scores) if raw_scores else 0\n",
    "        #min_score = min(raw_scores) if raw_scores else 0\n",
    "\n",
    "        # Avoid divide-by-zero\n",
    "        #if max_score != min_score:\n",
    "            #normalized_scores = [\n",
    "                #(sku, (score - min_score) / (max_score - min_score))\n",
    "                #for sku, score in scored_forgotten_skus\n",
    "                #]\n",
    "        #else:\n",
    "            #normalized_scores = [(sku, 0.0) for sku, _ in scored_forgotten_skus]\n",
    "\n",
    "        # Sort by frequency in descending order\n",
    "        sorted_forgotten_skus = sorted(scored_forgotten_skus, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "        # Append all forgotten skus with their frequency score\n",
    "        for sku, score in sorted_forgotten_skus:\n",
    "            recommendations.append({'Order': order_id, 'Member': member_id, 'SKU': sku, 'Frequency_Score': score})\n",
    "\n",
    "    out_df = pd.DataFrame(recommendations)\n",
    "    scaler = MinMaxScaler()\n",
    "    out_df[\"Frequency_Score_Scaled\"] = scaler.fit_transform(out_df[[\"Frequency_Score\"]])\n",
    "    return out_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOND8e96tBM2"
   },
   "source": [
    "### Seasonal recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rolling_popular_skus(last_orders_df, days=20, test_mode=False, k=5):\n",
    "    \"\"\"\n",
    "    Generates recommendations based on most popular SKUs in the last x days.\n",
    "    Input: Last orders in Dataframe format.\n",
    "    Output: Recommendations in Dataframe format.\n",
    "    \"\"\"\n",
    "\n",
    "    recommendations = []\n",
    "    df = df_input if test_mode else df_all\n",
    "\n",
    "    last_orders_df['Delivery Date'] = pd.to_datetime(last_orders_df['Delivery Date'])\n",
    "    df['Delivery Date'] = pd.to_datetime(df['Delivery Date'])\n",
    "    \n",
    "    cutoff_date = last_orders_df['Delivery Date'].max() - pd.Timedelta(days=days)\n",
    "    recent_orders = df[df['Delivery Date'] > cutoff_date]\n",
    "\n",
    "    for _, row in last_orders_df.iterrows():\n",
    "        member = row[\"Member\"]\n",
    "        order = row[\"Order\"]\n",
    "        target_date = row[\"Delivery Date\"]\n",
    "        cutoff_date = target_date - pd.Timedelta(days=days)\n",
    "        recent_orders = df[\n",
    "            (df['Delivery Date'] > cutoff_date) & \n",
    "            (df['Delivery Date'] <= target_date)\n",
    "        ]\n",
    "\n",
    "        sku_counts = (\n",
    "            recent_orders.groupby('SKU')\n",
    "            .size()\n",
    "            .reset_index(name='Seasonal_Score')\n",
    "            .sort_values(by='Seasonal_Score', ascending=False).head(k)\n",
    "        )\n",
    "\n",
    "        # Scale such that the score likes between 0 and 1\n",
    "        max_score = sku_counts['Seasonal_Score'].max()\n",
    "        min_score = sku_counts['Seasonal_Score'].min()\n",
    "        if max_score != min_score:\n",
    "            sku_counts['Seasonal_Score'] = (sku_counts['Seasonal_Score'] - min_score) / (max_score - min_score)\n",
    "        else:\n",
    "            sku_counts['Seasonal_Score'] = 0.0\n",
    "\n",
    "        for row in sku_counts.itertuples(index=False):\n",
    "            recommendations.append({\"Order\": order, \"Member\": member, \"SKU\": row.SKU, \"Seasonal_Score\": row.Seasonal_Score})\n",
    "\n",
    "    return pd.DataFrame(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "d640RvAxtEOc"
   },
   "outputs": [],
   "source": [
    "def seasonality_recommendations(last_orders_df, test_mode=False, k=5):\n",
    "    \"\"\"\n",
    "    Generates recommendations based on SKU seasonality.\n",
    "    Input: Last orders in Dataframe format.\n",
    "    Output: Recommendations in Dataframe format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get current month for each order\n",
    "    # last_orders_df[\"Month\"] = pd.to_datetime(last_orders_df[\"Delivery Date\"]).dt.month\n",
    "    last_orders_df[\"Season_Group\"] = pd.to_datetime(last_orders_df[\"Delivery Date\"]).dt.month.map(assign_season)\n",
    "\n",
    "    # Get the last observed order per member\n",
    "    df_for_iter = last_orders_df.groupby(\"Member\").last().reset_index()\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    for _, row in df_for_iter.iterrows():\n",
    "        member = row[\"Member\"]\n",
    "        order = row[\"Order\"]\n",
    "        current_month = row[\"Season_Group\"]\n",
    "\n",
    "        # Get all SKUs ever seen (can be filtered further later)\n",
    "        skus = sku_month_dist.index\n",
    "\n",
    "        # Extract seasonal score for this month\n",
    "        seasonal_scores = sku_month_dist.get(current_month)\n",
    "        if seasonal_scores is None:\n",
    "            seasonal_scores = pd.Series(0, index=sku_month_dist.index)\n",
    "\n",
    "        # Filter out already-in-cart SKUs\n",
    "        cart_skus = last_orders_df[last_orders_df[\"Order\"] == order][\"SKU\"].astype(str).tolist()\n",
    "        recs = (\n",
    "            seasonal_scores.drop(labels=cart_skus, errors=\"ignore\")\n",
    "            .sort_values(ascending=False)\n",
    "            .head(k)\n",
    "        )\n",
    "\n",
    "        for sku, score in recs.items():\n",
    "            recommendations.append({\"Order\": order, \"Member\": member, \"SKU\": sku, \"Month\": current_month, \"Seasonal_Score\": score})\n",
    "\n",
    "    return pd.DataFrame(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization - Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "Bmq9-GTGsowR"
   },
   "outputs": [],
   "source": [
    "def get_MF_recommendations(last_orders_df, k=2):\n",
    "    \"\"\"Generate top-k MF-based recommendations per member using Surprise SVD.\"\"\"\n",
    "\n",
    "    # 1. Prepare Surprise Dataset\n",
    "    df_interactions = last_orders_df[['Member', 'SKU']].copy()\n",
    "    df_interactions['rating'] = 1\n",
    "\n",
    "    reader = Reader(rating_scale=(0, 1))\n",
    "    data = Dataset.load_from_df(df_interactions[['Member', 'SKU', 'rating']], reader)\n",
    "    trainset = data.build_full_trainset()\n",
    "\n",
    "    # 2. Train SVD Model\n",
    "    algo = SVD(n_factors=50, biased=False)\n",
    "    algo.fit(trainset)\n",
    "\n",
    "    # 3. Get all members and items\n",
    "    all_members = df_interactions['Member'].unique()\n",
    "    all_items = df_interactions['SKU'].unique()\n",
    "\n",
    "    # 4. Avoid already interacted items\n",
    "    interacted = df_interactions.groupby('Member')['SKU'].apply(set).to_dict()\n",
    "\n",
    "    # 5. Generate predictions\n",
    "    recommendations = []\n",
    "\n",
    "    for member in all_members:\n",
    "        preds = []\n",
    "        known_items = interacted.get(member, set())\n",
    "\n",
    "        for item in all_items:\n",
    "            if item not in known_items:\n",
    "                est = algo.predict(member, item).est\n",
    "                preds.append((item, est))\n",
    "\n",
    "        # Scale the predicted scores\n",
    "        mf_scores = [score for _, score in preds]\n",
    "        max_score = max(mf_scores)\n",
    "        min_score = min(mf_scores)\n",
    "        if max_score != min_score:\n",
    "            normalized_preds = [(item, (score - min_score) / (max_score - min_score)) for item, score in preds]\n",
    "        else:\n",
    "            normalized_preds = [(item, 0.0) for item, _ in preds]\n",
    "\n",
    "        top_k = sorted(preds, key=lambda x: x[1], reverse=True)[:k]\n",
    "        \n",
    "        for item_id, mf_score in top_k:\n",
    "            recommendations.append({\n",
    "                \"Member\": member,\n",
    "                \"SKU\": item_id,\n",
    "                \"MF_score\": mf_score\n",
    "            })\n",
    "\n",
    "    # Create a lookup for Member → Order\n",
    "    member_order_map = last_orders_df.set_index('Member')['Order'].to_dict()\n",
    "\n",
    "    # Add Order info to each recommendation entry\n",
    "    for rec in recommendations:\n",
    "        rec['Order'] = member_order_map.get(rec['Member'])\n",
    "\n",
    "    return pd.DataFrame(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBsMdjSTnVI4"
   },
   "source": [
    "# 4. Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKNZIGCanVI5"
   },
   "source": [
    "### Key Steps\n",
    "- Create dataframes for model building\n",
    "  * From the all_except_last_orders dataset, remove the last order for each member. This becomes the new input.\n",
    "  * Create a new dataset, called last_orders, based on the last order for each member in all_except_last_orders dataset.\n",
    "  * From last_orders, remove a few SKUs for each order. This becomes our new last_orders_subset.\n",
    "  * The SKUs that were removed from the previous step, can be used to test recall@5.  \n",
    "- Functions are created in the strategy section.\n",
    "- Generate candidate SKUs, along with their scores, from multiple recommendation functions.\n",
    "- Rerank the candidates based on a combined score.\n",
    "- Ensure that the model output covers every order in \"last_orders_subset.csv\".\n",
    "- There should be exactly **5 unique SKUs**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da1cu5zxuGrf"
   },
   "source": [
    "### Create dataframes for model building\n",
    "This section only contains the code for creating new input data, observed data and withheld data from all_except_last_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2wNaGuM869N"
   },
   "source": [
    "Create a new input dataframe remove all but last order from all_except_last_orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "Uj3bAt6Vny0U"
   },
   "outputs": [],
   "source": [
    "# Group by Member and get the last Order for each Member\n",
    "last_orders_per_member = df_all.groupby('Member')['Order'].max().reset_index()\n",
    "last_orders_per_member = last_orders_per_member.rename(columns={'Order': 'Last_Order_ID'})\n",
    "\n",
    "# Merge with the original DataFrame to flag the last order rows\n",
    "df_all_with_last_flag = pd.merge(df_all, last_orders_per_member, on='Member', how='left')\n",
    "\n",
    "# Filter out rows where member has only one order.\n",
    "order_counts = df_all_with_last_flag.groupby('Member')['Order'].nunique()\n",
    "members_with_multiple_orders = order_counts[order_counts > 1].index\n",
    "df_all_with_last_flag = df_all_with_last_flag[df_all_with_last_flag['Member'].isin(members_with_multiple_orders)].copy()\n",
    "\n",
    "# Create df_input by filtering out the rows that are the last order for that member\n",
    "df_input = df_all_with_last_flag[df_all_with_last_flag['Order'] != df_all_with_last_flag['Last_Order_ID']].copy()\n",
    "\n",
    "# Drop the helper column\n",
    "df_input = df_input.drop(columns=['Last_Order_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "wc6YKxwLCICE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_input: (22232, 7)\n",
      "Verification of last order removal:\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order</th>\n",
       "      <th>SKU</th>\n",
       "      <th>Member</th>\n",
       "      <th>Delivery Date</th>\n",
       "      <th>Name</th>\n",
       "      <th>Month</th>\n",
       "      <th>Season_Group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8358896</td>\n",
       "      <td>15668375</td>\n",
       "      <td>SSCEHNS</td>\n",
       "      <td>2013-11-02</td>\n",
       "      <td>Root Vegetables</td>\n",
       "      <td>11</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8358896</td>\n",
       "      <td>15668467</td>\n",
       "      <td>SSCEHNS</td>\n",
       "      <td>2013-11-02</td>\n",
       "      <td>Beans</td>\n",
       "      <td>11</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8358896</td>\n",
       "      <td>15669863</td>\n",
       "      <td>SSCEHNS</td>\n",
       "      <td>2013-11-02</td>\n",
       "      <td>Moong Dal</td>\n",
       "      <td>11</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8358896</td>\n",
       "      <td>15669778</td>\n",
       "      <td>SSCEHNS</td>\n",
       "      <td>2013-11-02</td>\n",
       "      <td>Other Dals</td>\n",
       "      <td>11</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8358896</td>\n",
       "      <td>15669767</td>\n",
       "      <td>SSCEHNS</td>\n",
       "      <td>2013-11-02</td>\n",
       "      <td>Urad Dal</td>\n",
       "      <td>11</td>\n",
       "      <td>Winter</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Order       SKU   Member Delivery Date             Name  Month  \\\n",
       "0  8358896  15668375  SSCEHNS    2013-11-02  Root Vegetables     11   \n",
       "1  8358896  15668467  SSCEHNS    2013-11-02            Beans     11   \n",
       "2  8358896  15669863  SSCEHNS    2013-11-02        Moong Dal     11   \n",
       "3  8358896  15669778  SSCEHNS    2013-11-02       Other Dals     11   \n",
       "4  8358896  15669767  SSCEHNS    2013-11-02         Urad Dal     11   \n",
       "\n",
       "  Season_Group  \n",
       "0       Winter  \n",
       "1       Winter  \n",
       "2       Winter  \n",
       "3       Winter  \n",
       "4       Winter  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the new DataFrame\n",
    "print(f\"Shape of df_input: {df_input.shape}\")\n",
    "\n",
    "# Verify that for each member in df_input, their maximum order ID is less than the max order ID in df_all\n",
    "max_order_input = df_input.groupby('Member')['Order'].max().reset_index()\n",
    "max_order_all = df_all.groupby('Member')['Order'].max().reset_index()\n",
    "\n",
    "verification_df = pd.merge(max_order_input, max_order_all, on='Member', suffixes=('_input', '_all'))\n",
    "\n",
    "# Check if the max order in df_input is less than the max order in df_all for all members\n",
    "print(\"Verification of last order removal:\")\n",
    "print((verification_df['Order_input'] < verification_df['Order_all']).all())\n",
    "\n",
    "# Display the first few rows of df_input\n",
    "df_input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJOVt9sf9k49"
   },
   "source": [
    "* Select only the last orders and split the SKUs into observed and withheld.\n",
    "* The predictions will be made based on observed.\n",
    "* The recall@5 will be evaluated using withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "t0yqPt2_5T4P"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_last_orders_full (full last orders): (5610, 7)\n"
     ]
    }
   ],
   "source": [
    "# Select only the last orders (where Order == Last_Order_ID)\n",
    "df_last_orders_full = df_all_with_last_flag[df_all_with_last_flag['Order'] == df_all_with_last_flag['Last_Order_ID']].copy()\n",
    "df_last_orders_full = df_last_orders_full.drop(columns=['Last_Order_ID'])  # Drop the helper column\n",
    "\n",
    "# Check the shape\n",
    "print(f\"Shape of df_last_orders_full (full last orders): {df_last_orders_full.shape}\")\n",
    "\n",
    "# Apply the splitting function to each order\n",
    "split_results = df_last_orders_full.groupby(['Order', 'Member', 'Delivery Date'])['SKU'].apply(lambda x: split_order_skus(x, frac=0.3)).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "V97vCumgCzH_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of df_observed: (1461, 4)\n",
      "Shape of df_withheld: (2605, 4)\n"
     ]
    }
   ],
   "source": [
    "# Separate the results into two lists of tuples (Order, Member, SKU_list)\n",
    "observed_tuples = []\n",
    "withheld_tuples = []\n",
    "\n",
    "for _, row in split_results.iterrows():\n",
    "    order_id = row['Order']\n",
    "    member_id = row['Member']\n",
    "    observed_skus, withheld_skus = row['SKU']\n",
    "    delivery_date = row['Delivery Date']\n",
    "    observed_tuples.extend([(order_id, member_id, sku, delivery_date) for sku in observed_skus])\n",
    "    withheld_tuples.extend([(order_id, member_id, sku, delivery_date) for sku in withheld_skus])\n",
    "\n",
    "# Create the two DataFrames\n",
    "df_observed = pd.DataFrame(observed_tuples, columns=['Order', 'Member', 'SKU', 'Delivery Date'])\n",
    "df_withheld = pd.DataFrame(withheld_tuples, columns=['Order', 'Member', 'SKU', 'Delivery Date'])\n",
    "\n",
    "print(f\"Shape of df_observed: {df_observed.shape}\")\n",
    "print(f\"Shape of df_withheld: {df_withheld.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "giz1CKfmCrXx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total original SKUs in last orders: 5610\n",
      "Total SKUs in observed and withheld: 4066\n",
      "Verification: Total split SKUs match original: False\n"
     ]
    }
   ],
   "source": [
    "# Verify the split\n",
    "# Calculate total original SKUs\n",
    "total_original_skus = df_last_orders_full.shape[0]\n",
    "\n",
    "# Calculate total SKUs in split dataframes\n",
    "total_split_skus = df_observed.shape[0] + df_withheld.shape[0]\n",
    "print(f\"Total original SKUs in last orders: {total_original_skus}\")\n",
    "print(f\"Total SKUs in observed and withheld: {total_split_skus}\")\n",
    "print(f\"Verification: Total split SKUs match original: {total_original_skus == total_split_skus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "cs5joIuhCiQ9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of orders in full last orders: 521\n",
      "Number of orders in df_observed: 521\n",
      "Number of orders in df_withheld: 521\n",
      "\n",
      "Head of df_observed:\n",
      "     Order   Member       SKU Delivery Date\n",
      "0  7387496  SSZWOEE  15668381    2013-10-01\n",
      "1  7387496  SSZWOEE  15668460    2013-10-01\n",
      "2  7395007  SSNSCNE  15670251    2013-10-05\n",
      "3  7395007  SSNSCNE  15668379    2013-10-05\n",
      "4  7408892  SSLSWRE  15669821    2013-09-06\n",
      "\n",
      "Head of df_withheld:\n",
      "     Order   Member       SKU Delivery Date\n",
      "0  7387496  SSZWOEE  15668477    2013-10-01\n",
      "1  7387496  SSZWOEE  92388167    2013-10-01\n",
      "2  7387496  SSZWOEE  15668462    2013-10-01\n",
      "3  7387496  SSZWOEE  15669817    2013-10-01\n",
      "4  7387496  SSZWOEE  15668378    2013-10-01\n"
     ]
    }
   ],
   "source": [
    "# Check if all orders are present in both (unless an order had only 1 SKU, in which case it's only in observed)\n",
    "orders_observed = set(df_observed['Order'].unique())\n",
    "orders_withheld = set(df_withheld['Order'].unique())\n",
    "orders_full = set(df_last_orders_full['Order'].unique())\n",
    "\n",
    "print(f\"Number of orders in full last orders: {len(orders_full)}\")\n",
    "print(f\"Number of orders in df_observed: {len(orders_observed)}\")\n",
    "print(f\"Number of orders in df_withheld: {len(orders_withheld)}\")  # This will be less if orders had only 1 SKU\n",
    "\n",
    "# Display head of the new dataframes\n",
    "print(\"\\nHead of df_observed:\")\n",
    "print(df_observed.head())\n",
    "print(\"\\nHead of df_withheld:\")\n",
    "print(df_withheld.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-35-T3Ptgwj"
   },
   "source": [
    "### Apply Member Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "id": "ae5DI9FXtXpM"
   },
   "outputs": [],
   "source": [
    "df_last_observed = pd.merge(df_observed, member_stats_df[['Member', 'Member_Cluster']], on='Member', how='left')\n",
    "\n",
    "clusters = ['Repetitive', 'Explorer', 'NewUser', 'FrequentBuyer', 'Normal']\n",
    "df_observed_list = [df_last_observed[df_last_observed['Member_Cluster'] == cluster] for cluster in clusters]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-zde172OoJM"
   },
   "source": [
    "### Get recommendations using df_observed and evaluate recall@5 using df_withheld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "id": "jo195gB9gm9n"
   },
   "outputs": [],
   "source": [
    "strategies = [\n",
    "    lambda df: jaccard_recommendations(df, global_jaccard_matrix, test_mode=True),\n",
    "    lambda df: jaccard_cart_reranker(df, test_mode=True),\n",
    "    lambda df: history_based_recommendations(df, test_mode=True),\n",
    "    lambda df: periodicity_reranker(df, test_mode=True),\n",
    "    lambda df: get_rolling_popular_skus(df, test_mode=True),\n",
    "    lambda df: get_MF_recommendations(df)\n",
    "]\n",
    "\n",
    "strategy_names = ['Jaccard', 'MemJaccard', 'History', 'Periodicity', 'Seasonality', 'Surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "id": "OH-ZGjV2gvA0"
   },
   "outputs": [],
   "source": [
    "df_preds = []  # Will contain all 15 recommendation DataFrames\n",
    "pred_labels = []  # To track which cluster and strategy\n",
    "\n",
    "for i, df_cluster in enumerate(df_observed_list):\n",
    "    for j, strategy in enumerate(strategies):\n",
    "        df_pred = strategy(df_cluster)\n",
    "        df_preds.append(df_pred)\n",
    "        pred_labels.append(f\"{clusters[i]}_{strategy_names[j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "id": "LK3Tv3kP9XbJ"
   },
   "outputs": [],
   "source": [
    "df_pred_dict = dict(zip(pred_labels, df_preds))\n",
    "\n",
    "# Organize predictions per cluster\n",
    "cluster_preds = defaultdict(list)\n",
    "\n",
    "for label, df in zip(pred_labels, df_preds):\n",
    "    cluster = label.split(\"_\")[0]  # 'Explorer', 'Repetitive', etc.\n",
    "    cluster_preds[cluster].append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: Repetitive\n",
      "  Strategy 0: shape=(804, 4), columns=['Member', 'Order', 'SKU', 'Global_Jaccard_Score']\n",
      "  Strategy 1: shape=(803, 4), columns=['Order', 'Member', 'SKU', 'Member_Jaccard_Score']\n",
      "  Strategy 2: shape=(1002, 5), columns=['Order', 'Member', 'SKU', 'Frequency_Score', 'Frequency_Score_Scaled']\n",
      "  Strategy 3: shape=(536, 4), columns=['Order', 'Member', 'SKU', 'Periodicity_Score']\n",
      "  Strategy 4: shape=(1105, 4), columns=['Order', 'Member', 'SKU', 'Seasonal_Score']\n",
      "  Strategy 5: shape=(134, 4), columns=['Member', 'SKU', 'MF_score', 'Order']\n",
      "Cluster: Explorer\n",
      "  Strategy 0: shape=(1608, 4), columns=['Member', 'Order', 'SKU', 'Global_Jaccard_Score']\n",
      "  Strategy 1: shape=(946, 4), columns=['Order', 'Member', 'SKU', 'Member_Jaccard_Score']\n",
      "  Strategy 2: shape=(1987, 5), columns=['Order', 'Member', 'SKU', 'Frequency_Score', 'Frequency_Score_Scaled']\n",
      "  Strategy 3: shape=(274, 4), columns=['Order', 'Member', 'SKU', 'Periodicity_Score']\n",
      "  Strategy 4: shape=(1780, 4), columns=['Order', 'Member', 'SKU', 'Seasonal_Score']\n",
      "  Strategy 5: shape=(268, 4), columns=['Member', 'SKU', 'MF_score', 'Order']\n",
      "Cluster: NewUser\n",
      "  Strategy 0: shape=(1584, 4), columns=['Member', 'Order', 'SKU', 'Global_Jaccard_Score']\n",
      "  Strategy 1: shape=(597, 4), columns=['Order', 'Member', 'SKU', 'Member_Jaccard_Score']\n",
      "  Strategy 2: shape=(1216, 5), columns=['Order', 'Member', 'SKU', 'Frequency_Score', 'Frequency_Score_Scaled']\n",
      "  Strategy 3: shape=(132, 4), columns=['Order', 'Member', 'SKU', 'Periodicity_Score']\n",
      "  Strategy 4: shape=(1690, 4), columns=['Order', 'Member', 'SKU', 'Seasonal_Score']\n",
      "  Strategy 5: shape=(264, 4), columns=['Member', 'SKU', 'MF_score', 'Order']\n",
      "Cluster: FrequentBuyer\n",
      "  Strategy 0: shape=(1164, 4), columns=['Member', 'Order', 'SKU', 'Global_Jaccard_Score']\n",
      "  Strategy 1: shape=(1060, 4), columns=['Order', 'Member', 'SKU', 'Member_Jaccard_Score']\n",
      "  Strategy 2: shape=(1455, 5), columns=['Order', 'Member', 'SKU', 'Frequency_Score', 'Frequency_Score_Scaled']\n",
      "  Strategy 3: shape=(753, 4), columns=['Order', 'Member', 'SKU', 'Periodicity_Score']\n",
      "  Strategy 4: shape=(1445, 4), columns=['Order', 'Member', 'SKU', 'Seasonal_Score']\n",
      "  Strategy 5: shape=(194, 4), columns=['Member', 'SKU', 'MF_score', 'Order']\n",
      "Cluster: Normal\n",
      "  Strategy 0: shape=(1092, 4), columns=['Member', 'Order', 'SKU', 'Global_Jaccard_Score']\n",
      "  Strategy 1: shape=(954, 4), columns=['Order', 'Member', 'SKU', 'Member_Jaccard_Score']\n",
      "  Strategy 2: shape=(1336, 5), columns=['Order', 'Member', 'SKU', 'Frequency_Score', 'Frequency_Score_Scaled']\n",
      "  Strategy 3: shape=(548, 4), columns=['Order', 'Member', 'SKU', 'Periodicity_Score']\n",
      "  Strategy 4: shape=(1285, 4), columns=['Order', 'Member', 'SKU', 'Seasonal_Score']\n",
      "  Strategy 5: shape=(182, 4), columns=['Member', 'SKU', 'MF_score', 'Order']\n"
     ]
    }
   ],
   "source": [
    "for cluster, dfs in cluster_preds.items():\n",
    "    print(f\"Cluster: {cluster}\")\n",
    "    for i, df in enumerate(dfs):\n",
    "        print(f\"  Strategy {i}: shape={df.shape}, columns={df.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "id": "-aXirJRCjTLk"
   },
   "outputs": [],
   "source": [
    "combined_preds = {}\n",
    "\n",
    "for cluster, dfs in cluster_preds.items():\n",
    "    merged_df = dfs[0]\n",
    "\n",
    "    for df in dfs[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, on=['Member','Order','SKU'], how='outer')\n",
    "        merged_df = merged_df.drop_duplicates(subset=['Member', 'SKU'], keep='first')\n",
    "        \n",
    "    combined_preds[cluster] = merged_df\n",
    "\n",
    "combined_preds['Normal'] = combined_preds['Normal'].fillna(0)\n",
    "combined_preds['Explorer'] = combined_preds['Explorer'].fillna(0)\n",
    "combined_preds['Repetitive'] = combined_preds['Repetitive'].fillna(0)\n",
    "combined_preds['FrequentBuyer'] = combined_preds['FrequentBuyer'].fillna(0)\n",
    "combined_preds['NewUser'] = combined_preds['NewUser'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Level        Segment  Recall@5\n",
      "0  Cluster     Repetitive    0.8358\n",
      "4  Cluster         Normal    0.6901\n",
      "3  Cluster  FrequentBuyer    0.6742\n",
      "5  Overall            All    0.5923\n",
      "2  Cluster        NewUser    0.4727\n",
      "1  Cluster       Explorer    0.4627\n"
     ]
    }
   ],
   "source": [
    "# Check how good the candidate SKUs are.\n",
    "# Recall will be higher because the filtering is not applied.\n",
    "recall_df = run_recall_evaluation(combined_preds, df_withheld)\n",
    "print(recall_df.sort_values(by='Recall@5', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "id": "ZVwILHrDDMDo"
   },
   "outputs": [],
   "source": [
    "weights_rep = {'Jaccard': 0.1, 'MemJaccard' : 0.3, 'History': 0.6, 'Periodicity': 0.0, 'Seasonality': 0.0, 'Surprise': 0.0}\n",
    "\n",
    "combined_preds['Repetitive']['Combined_Score'] = (\n",
    "    weights_rep['Jaccard'] * combined_preds['Repetitive']['Global_Jaccard_Score'] +\n",
    "    weights_rep['MemJaccard'] * combined_preds['Repetitive']['Member_Jaccard_Score'] +\n",
    "    weights_rep['History'] * combined_preds['Repetitive']['Frequency_Score'] +\n",
    "    weights_rep['Periodicity'] * combined_preds['Repetitive']['Periodicity_Score'] +\n",
    "    weights_rep['Seasonality'] * combined_preds['Repetitive']['Seasonal_Score'] +\n",
    "    weights_rep['Surprise'] * combined_preds['Repetitive']['MF_score']\n",
    ")\n",
    "\n",
    "combined_preds['Repetitive'] = (\n",
    "    combined_preds['Repetitive']\n",
    "    .sort_values(by=['Member', 'Combined_Score'], ascending=[True, False])\n",
    "    .groupby('Member')\n",
    "    .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "id": "ITUplRf1Pb1q"
   },
   "outputs": [],
   "source": [
    "weights_norm = {'Jaccard': 0.2, 'MemJaccard' : 0.25, 'History': 0.45, 'Periodicity': 0.05, 'Seasonality': 0.05, 'Surprise': 0.0}\n",
    "\n",
    "combined_preds['Normal']['Combined_Score'] = (\n",
    "    weights_norm['Jaccard'] * combined_preds['Normal']['Global_Jaccard_Score'] +\n",
    "    weights_norm['MemJaccard'] * combined_preds['Normal']['Member_Jaccard_Score'] +\n",
    "    weights_norm['History'] * combined_preds['Normal']['Frequency_Score'] +\n",
    "    weights_norm['Periodicity'] * combined_preds['Normal']['Periodicity_Score'] +\n",
    "    weights_norm['Seasonality'] * combined_preds['Normal']['Seasonal_Score'] +\n",
    "    weights_norm['Surprise'] * combined_preds['Normal']['MF_score']\n",
    ")\n",
    "\n",
    "combined_preds['Normal'] = (\n",
    "    combined_preds['Normal']\n",
    "    .sort_values(by=['Member', 'Combined_Score'], ascending=[True, False])\n",
    "    .groupby('Member')\n",
    "    .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_freq = {'Jaccard': 0.0, 'MemJaccard' : 0.4, 'History': 0.5, 'Periodicity': 0.1, 'Seasonality': 0.0, 'Surprise': 0.0}\n",
    "\n",
    "combined_preds['FrequentBuyer']['Combined_Score'] = (\n",
    "    weights_freq['Jaccard'] * combined_preds['FrequentBuyer']['Global_Jaccard_Score'] +\n",
    "    weights_freq['MemJaccard'] * combined_preds['FrequentBuyer']['Member_Jaccard_Score'] +\n",
    "    weights_freq['History'] * combined_preds['FrequentBuyer']['Frequency_Score'] +\n",
    "    weights_freq['Periodicity'] * combined_preds['FrequentBuyer']['Periodicity_Score'] +\n",
    "    weights_freq['Seasonality'] * combined_preds['FrequentBuyer']['Seasonal_Score'] +\n",
    "    weights_freq['Surprise'] * combined_preds['FrequentBuyer']['MF_score']\n",
    ")\n",
    "\n",
    "combined_preds['FrequentBuyer'] = (\n",
    "    combined_preds['FrequentBuyer']\n",
    "    .sort_values(by=['Member', 'Combined_Score'], ascending=[True, False])\n",
    "    .groupby('Member')\n",
    "    .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_newUser = {'Jaccard': 0.35, 'MemJaccard' : 0.1, 'History': 0.55, 'Periodicity': 0.0, 'Seasonality': 0.0, 'Surprise': 0.0}\n",
    "\n",
    "combined_preds['NewUser']['Combined_Score'] = (\n",
    "    weights_newUser['Jaccard'] * combined_preds['NewUser']['Global_Jaccard_Score'] +\n",
    "    weights_newUser['MemJaccard'] * combined_preds['NewUser']['Member_Jaccard_Score'] +\n",
    "    weights_newUser['History'] * combined_preds['NewUser']['Frequency_Score'] +\n",
    "    weights_newUser['Periodicity'] * combined_preds['NewUser']['Periodicity_Score'] +\n",
    "    weights_newUser['Seasonality'] * combined_preds['NewUser']['Seasonal_Score'] +\n",
    "    weights_newUser['Surprise'] * combined_preds['NewUser']['MF_score']\n",
    ")\n",
    "\n",
    "combined_preds['NewUser'] = (\n",
    "    combined_preds['NewUser']\n",
    "    .sort_values(by=['Member', 'Combined_Score'], ascending=[True, False])\n",
    "    .groupby('Member')\n",
    "    .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "id": "EGKL___0IVm2"
   },
   "outputs": [],
   "source": [
    "weights_exp = {'Jaccard': 0.3, 'MemJaccard' : 0.25, 'History': 0.45, 'Periodicity': 0.0, 'Seasonality': 0.0, 'Surprise': 0.0}\n",
    "\n",
    "combined_preds['Explorer']['Combined_Score'] = (\n",
    "    weights_exp['Jaccard'] * combined_preds['Explorer']['Global_Jaccard_Score'] +\n",
    "    weights_exp['MemJaccard'] * combined_preds['Explorer']['Member_Jaccard_Score'] +\n",
    "    weights_exp['History'] * combined_preds['Explorer']['Frequency_Score'] +\n",
    "    weights_exp['Periodicity'] * combined_preds['Explorer']['Periodicity_Score'] +\n",
    "    weights_exp['Seasonality'] * combined_preds['Explorer']['Seasonal_Score'] +\n",
    "    weights_exp['Surprise'] * combined_preds['Explorer']['MF_score']\n",
    ")\n",
    "\n",
    "combined_preds['Explorer'] = (\n",
    "    combined_preds['Explorer']\n",
    "    .sort_values(by=['Member', 'Combined_Score'], ascending=[True, False])\n",
    "    .groupby('Member')\n",
    "    .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Level        Segment  Recall@5\n",
      "4  Cluster         Normal    0.3495\n",
      "0  Cluster     Repetitive    0.3045\n",
      "3  Cluster  FrequentBuyer    0.2907\n",
      "5  Overall            All    0.2583\n",
      "2  Cluster        NewUser    0.2227\n",
      "1  Cluster       Explorer    0.1851\n"
     ]
    }
   ],
   "source": [
    "# Check the recall after ranking\n",
    "recall_df = run_recall_evaluation(combined_preds, df_withheld)\n",
    "print(recall_df.sort_values(by='Recall@5', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Member    Order       SKU  Predicted_Rank\n",
      "2089  SWCEWOZ  8383034  15668478               1\n",
      "1446  SSWECLS  7513293  15669957               1\n",
      "1463  SSWERSN  7540624  15668473               1\n",
      "1523  SSWLNWS  8375902  15668460               1\n",
      "1547  SSWNLNH  8286266  15668468               1\n",
      "1593  SSWNOEH  7565384  15669878               1\n",
      "1648  SSWZRHL  7539082  15668468               1\n",
      "1815  SSZLHHE  7561456  15669878               1\n",
      "1825  SSZRSZL  8376951  15668458               1\n",
      "1996  SWCCZRH  8378882  15668381               1\n"
     ]
    }
   ],
   "source": [
    "hits_df = position_of_truth(combined_preds['Explorer'], df_withheld)\n",
    "print(hits_df.sort_values(by='Predicted_Rank').head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i13HVEY9todv"
   },
   "source": [
    "### Apply prediction algorithms to original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_orders = df_last.copy()\n",
    "df_last_orders = pd.merge(df_last, member_stats_df[['Member', 'Member_Cluster']], on='Member', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last_orders_clustered = [df_last_orders[df_last_orders['Member_Cluster'] == cluster] for cluster in clusters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategies = [\n",
    "    lambda df: jaccard_recommendations(df, global_jaccard_matrix),\n",
    "    lambda df: jaccard_cart_reranker(df),\n",
    "    lambda df: history_based_recommendations(df),\n",
    "    lambda df: periodicity_reranker(df),\n",
    "    lambda df: get_rolling_popular_skus(df),\n",
    "    lambda df: get_MF_recommendations(df)\n",
    "]\n",
    "\n",
    "strategy_names = ['Jaccard', 'MemJaccard', 'History', 'Periodicity', 'Seasonality', 'Surprise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preds = []  # Will contain all 15 recommendation DataFrames\n",
    "pred_labels = []  # To track which cluster and strategy\n",
    "\n",
    "for i, df_cluster in enumerate(df_last_orders_clustered):\n",
    "    for j, strategy in enumerate(strategies):\n",
    "        df_pred = strategy(df_cluster)\n",
    "        df_preds.append(df_pred)\n",
    "        pred_labels.append(f\"{clusters[i]}_{strategy_names[j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_dict = dict(zip(pred_labels, df_preds))\n",
    "\n",
    "# Organize predictions per cluster\n",
    "cluster_preds = defaultdict(list)\n",
    "\n",
    "for label, df in zip(pred_labels, df_preds):\n",
    "    cluster = label.split(\"_\")[0]  # 'Explorer', 'Repetitive', etc.\n",
    "    cluster_preds[cluster].append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {
    "id": "NYSmnoWRM50C"
   },
   "outputs": [],
   "source": [
    "combined_preds = {}\n",
    "\n",
    "for cluster, dfs in cluster_preds.items():\n",
    "    merged_df = dfs[0]\n",
    "\n",
    "    for df in dfs[1:]:\n",
    "        merged_df = pd.merge(merged_df, df, on=['Member','Order','SKU'], how='outer')\n",
    "        merged_df = merged_df.drop_duplicates(subset=['Member', 'SKU'], keep='first')\n",
    "        \n",
    "    combined_preds[cluster] = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "metadata": {
    "id": "9DOrS1SGGg6O"
   },
   "outputs": [],
   "source": [
    "combined_preds['Normal'] = combined_preds['Normal'].fillna(0)\n",
    "combined_preds['Explorer'] = combined_preds['Explorer'].fillna(0)\n",
    "combined_preds['Repetitive'] = combined_preds['Repetitive'].fillna(0)\n",
    "combined_preds['FrequentBuyer'] = combined_preds['FrequentBuyer'].fillna(0)\n",
    "combined_preds['NewUser'] = combined_preds['NewUser'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "metadata": {
    "id": "mhFd0ZyvuzQo"
   },
   "outputs": [],
   "source": [
    "# weights_rep gets picked from the last tested data\n",
    "weights_rep = {'Jaccard': 0.1, 'MemJaccard' : 0.3, 'History': 0.6, 'Periodicity': 0.0, 'Seasonality': 0.0, 'Surprise': 0.0}\n",
    "\n",
    "combined_preds['Repetitive']['Combined_Score'] = (\n",
    "    weights_rep['Jaccard'] * combined_preds['Repetitive']['Global_Jaccard_Score'] +\n",
    "    weights_rep['MemJaccard'] * combined_preds['Repetitive']['Member_Jaccard_Score'] +\n",
    "    weights_rep['History'] * combined_preds['Repetitive']['Frequency_Score'] +\n",
    "    weights_rep['Periodicity'] * combined_preds['Repetitive']['Periodicity_Score'] +\n",
    "    weights_rep['Seasonality'] * combined_preds['Repetitive']['Seasonal_Score'] +\n",
    "    weights_rep['Surprise'] * combined_preds['Repetitive']['MF_score']\n",
    ")\n",
    "\n",
    "combined_preds['Repetitive'] = (\n",
    "    combined_preds['Repetitive']\n",
    "    .sort_values(by=['Member', 'Combined_Score'], ascending=[True, False])\n",
    "    .groupby('Member')\n",
    "    .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_norm gets picked from the last tested data\n",
    "weights_norm = {'Jaccard': 0.2, 'MemJaccard' : 0.3, 'History': 0.5, 'Periodicity': 0.0, 'Seasonality': 0.0, 'Surprise': 0.0}\n",
    "\n",
    "combined_preds['Normal']['Combined_Score'] = (\n",
    "    weights_norm['Jaccard'] * combined_preds['Normal']['Global_Jaccard_Score'] +\n",
    "    weights_norm['MemJaccard'] * combined_preds['Normal']['Member_Jaccard_Score'] +\n",
    "    weights_norm['History'] * combined_preds['Normal']['Frequency_Score'] +\n",
    "    weights_norm['Periodicity'] * combined_preds['Normal']['Periodicity_Score'] +\n",
    "    weights_norm['Seasonality'] * combined_preds['Normal']['Seasonal_Score'] +\n",
    "    weights_norm['Surprise'] * combined_preds['Normal']['MF_score']\n",
    ")\n",
    "\n",
    "combined_preds['Normal'] = (\n",
    "    combined_preds['Normal']\n",
    "    .sort_values(by=['Member', 'Combined_Score'], ascending=[True, False])\n",
    "    .groupby('Member')\n",
    "    .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_freq gets picked from the last tested data\n",
    "weights_freq = {'Jaccard': 0.0, 'MemJaccard' : 0.4, 'History': 0.6, 'Periodicity': 0.0, 'Seasonality': 0.0, 'Surprise': 0.0}\n",
    "\n",
    "combined_preds['FrequentBuyer']['Combined_Score'] = (\n",
    "    weights_freq['Jaccard'] * combined_preds['FrequentBuyer']['Global_Jaccard_Score'] +\n",
    "    weights_freq['MemJaccard'] * combined_preds['FrequentBuyer']['Member_Jaccard_Score'] +\n",
    "    weights_freq['History'] * combined_preds['FrequentBuyer']['Frequency_Score'] +\n",
    "    weights_freq['Periodicity'] * combined_preds['FrequentBuyer']['Periodicity_Score'] +\n",
    "    weights_freq['Seasonality'] * combined_preds['FrequentBuyer']['Seasonal_Score'] +\n",
    "    weights_freq['Surprise'] * combined_preds['FrequentBuyer']['MF_score']\n",
    ")\n",
    "\n",
    "combined_preds['FrequentBuyer'] = (\n",
    "    combined_preds['FrequentBuyer']\n",
    "    .sort_values(by=['Member', 'Combined_Score'], ascending=[True, False])\n",
    "    .groupby('Member')\n",
    "    .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_newUser gets picked from the last tested data\n",
    "weights_newUser = {'Jaccard': 0.35, 'MemJaccard' : 0.0, 'History': 0.65, 'Periodicity': 0.0, 'Seasonality': 0.0, 'Surprise': 0.0}\n",
    "\n",
    "combined_preds['NewUser']['Combined_Score'] = (\n",
    "    weights_newUser['Jaccard'] * combined_preds['NewUser']['Global_Jaccard_Score'] +\n",
    "    weights_newUser['MemJaccard'] * combined_preds['NewUser']['Member_Jaccard_Score'] +\n",
    "    weights_newUser['History'] * combined_preds['NewUser']['Frequency_Score'] +\n",
    "    weights_newUser['Periodicity'] * combined_preds['NewUser']['Periodicity_Score'] +\n",
    "    weights_newUser['Seasonality'] * combined_preds['NewUser']['Seasonal_Score'] +\n",
    "    weights_newUser['Surprise'] * combined_preds['NewUser']['MF_score']\n",
    ")\n",
    "\n",
    "combined_preds['NewUser'] = (\n",
    "    combined_preds['NewUser']\n",
    "    .sort_values(by=['Member', 'Combined_Score'], ascending=[True, False])\n",
    "    .groupby('Member')\n",
    "    .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_exp gets picked from the last tested data\n",
    "weights_exp = {'Jaccard': 0.25, 'MemJaccard' : 0.25, 'History': 0.5, 'Periodicity': 0.0, 'Seasonality': 0.0, 'Surprise': 0.0}\n",
    "\n",
    "combined_preds['Explorer']['Combined_Score'] = (\n",
    "    weights_exp['Jaccard'] * combined_preds['Explorer']['Global_Jaccard_Score'] +\n",
    "    weights_exp['History'] * combined_preds['Explorer']['Frequency_Score'] +\n",
    "    weights_exp['Periodicity'] * combined_preds['Explorer']['Periodicity_Score'] +\n",
    "    weights_exp['Seasonality'] * combined_preds['Explorer']['Seasonal_Score'] +\n",
    "    weights_exp['Surprise'] * combined_preds['Explorer']['MF_score']\n",
    ")\n",
    "\n",
    "combined_preds['Explorer'] = (\n",
    "    combined_preds['Explorer']\n",
    "    .sort_values(by=['Member', 'Combined_Score'], ascending=[True, False])\n",
    "    .groupby('Member')\n",
    "    .head(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([\n",
    "    combined_preds['Explorer'],\n",
    "    combined_preds['Repetitive'],\n",
    "    combined_preds['Normal'],\n",
    "    combined_preds['FrequentBuyer'],\n",
    "    combined_preds['NewUser']\n",
    "], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {
    "id": "gvrw9RKQv4fh"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3190"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by priority, then slice top 5 per order\n",
    "df_final = (\n",
    "    df_combined\n",
    "    .sort_values(by=['Order', 'Combined_Score'], ascending=[True, False])\n",
    "    .groupby('Order')\n",
    "    .head(5)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "len(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jx_APeIonVI5"
   },
   "source": [
    "# 5. Submission file creation\n",
    "### Format\n",
    "- CSV file with 4 columns: ID, Order, SKU, Member.\n",
    "- Exactly 5 rows per order.\n",
    "- There are 638 unique orders in \"last_orders_subset.csv\". The csv file should contain 638 * 5 = 3190 rows.\n",
    "- No duplicate members.\n",
    "- File should be named: **GR_12_rec_5_sets.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {
    "id": "fWeXUSL3nVI5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    638.0\n",
       "mean       5.0\n",
       "std        0.0\n",
       "min        5.0\n",
       "25%        5.0\n",
       "50%        5.0\n",
       "75%        5.0\n",
       "max        5.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write submission file\n",
    "df_submission = df_final.copy()\n",
    "assert len(df_submission) == 3190, \"Number of rows doesn't match expected count.\"\n",
    "\n",
    "# Add the ID column if it hasn't been added yet\n",
    "df_submission.insert(0, 'ID', range(1, len(df_submission) + 1))\n",
    "\n",
    "# Reorder the columns\n",
    "desired_order = ['ID', 'Order', 'SKU', 'Member']\n",
    "df_submission = df_submission[desired_order]\n",
    "\n",
    "df_submission.to_csv(\"GR12_rec_5_sets.csv\", index=False)\n",
    "\n",
    "# Ensure that there are exactly 5 rows for each order\n",
    "df_submission.groupby('Order').size().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-TsERa4nVI5"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50b78775"
   },
   "source": [
    "# PEP8 Compliance Changes\n",
    "\n",
    "The following changes were made to improve PEP8 compliance:\n",
    "\n",
    "- Added blank lines around function definitions and class definitions.\n",
    "- Added blank lines within functions to separate logical sections.\n",
    "- Removed unnecessary blank lines.\n",
    "- Corrected indentation where needed.\n",
    "- Added or adjusted comments for clarity.\n",
    "- Ensured consistent spacing around operators and in function calls.\n",
    "- Ensured consistent naming conventions (variables, functions).\n",
    "- Added docstrings to functions where appropriate.\n",
    "- Wrapped long lines to improve readability (although some long lines with data or print statements were left as is for clarity in this context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12792058,
     "sourceId": 105745,
     "sourceType": "competition"
    },
    {
     "datasetId": 7784878,
     "sourceId": 12348609,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
